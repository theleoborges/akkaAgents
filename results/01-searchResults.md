# TITLE: Research: quantifying GitHub Copilot’s impact on developer productivity and happiness - The GitHub Blog
# URL: https://github.blog/news-insights/research/research-quantifying-github-copilots-impact-on-developer-productivity-and-happiness/
# CONTENT:
Research: quantifying GitHub Copilot’s impact on developer productivity and happiness - The GitHub Blog Skip to content Skip to sidebar / Blog Changelog Docs Customer stories Use Copilot for free Contact sales AI & ML AI & ML Learn about artificial intelligence and machine learning across the GitHub ecosystem and the wider industry. Generative AI Learn how to build with generative AI. GitHub Copilot Change how you work with GitHub Copilot. LLMs Everything developers need to know about LLMs. Machine learning Machine learning tips, tricks, and best practices. How AI code generation works Explore the capabilities and benefits of AI code generation and how it can improve your developer experience. Learn more Developer skills Developer skills Resources for developers to grow in their skills and careers. Application development Insights and best practices for building apps. Career growth Tips & tricks to grow as a professional developer. GitHub Improve how you use GitHub at work. GitHub Education Learn how to move into your first professional role. Programming languages & frameworks Stay current on what’s new (or new again). Get started with GitHub documentation Learn how to start building, shipping, and maintaining software with GitHub. Learn more Engineering Engineering Get an inside look at how we’re building the home for all developers. Architecture & optimization Discover how we deliver a performant and highly available experience across the GitHub platform. Engineering principles Explore best practices for building software at scale with a majority remote team. Infrastructure Get a glimpse at the technology underlying the world’s leading AI-powered developer platform. Platform security Learn how we build security into everything we do across the developer lifecycle. User experience Find out what goes into making GitHub the home for all developers. How we use GitHub to be more productive, collaborative, and secure Our engineering and security teams do some incredible work. Let’s take a look at how we use GitHub to be more productive, build collaboratively, and shift security left. Learn more Enterprise software Enterprise software Explore how to write, build, and deploy enterprise software at scale. Automation Automating your way to faster and more secure ships. CI/CD Guides on continuous integration and delivery. Collaboration Tips, tools, and tricks to improve developer collaboration. DevOps DevOps resources for enterprise engineering teams. DevSecOps How to integrate security into the SDLC. Governance & compliance Ensuring your builds stay clean. How enterprise engineering teams can successfully adopt AI Learn how to bring AI to your engineering teams and maximize the value that you get from it. Learn more News & insights News & insights Keep up with what’s new and notable from inside GitHub. Company news An inside look at news and product updates from GitHub. Product The latest on GitHub’s platform, products, and tools. Octoverse Insights into the state of open source on GitHub. Policy The latest policy and regulatory changes in software. Research Data-driven insights around the developer ecosystem. The library Older news and updates from GitHub. Unlocking the power of unstructured data with RAG Learn how to use retrieval-augmented generation (RAG) to capture more insights. Learn more Open Source Open Source Everything open source on GitHub. Git The latest Git updates. Maintainers Spotlighting open source maintainers. Social impact How open source is driving positive change. Gaming Explore open source games on GitHub. An introduction to innersource Organizations worldwide are incorporating open source methodologies into the way they build and ship their own software. Learn more Security Security Stay up to date on everything security. Application security Application security, explained. Supply chain security Demystifying supply chain security. Vulnerability research Updates from the GitHub Security Lab. Web application security Helpful tips on securing web applications. The enterprise guide to AI-powered DevSecOps Learn about core challenges in DevSecOps, and how you can start addressing them with AI and automation. Learn more Search Categories AI & ML Back AI & ML Learn about artificial intelligence and machine learning across the GitHub ecosystem and the wider industry. Generative AI Learn how to build with generative AI. GitHub Copilot Change how you work with GitHub Copilot. LLMs Everything developers need to know about LLMs. Machine learning Machine learning tips, tricks, and best practices. How AI code generation works Explore the capabilities and benefits of AI code generation and how it can improve your developer experience. Learn more Developer skills Back Developer skills Resources for developers to grow in their skills and careers. Application development Insights and best practices for building apps. Career growth Tips & tricks to grow as a professional developer. GitHub Improve how you use GitHub at work. GitHub Education Learn how to move into your first professional role. Programming languages & frameworks Stay current on what’s new (or new again). Get started with GitHub documentation Learn how to start building, shipping, and maintaining software with GitHub. Learn more Engineering Back Engineering Get an inside look at how we’re building the home for all developers. Architecture & optimization Discover how we deliver a performant and highly available experience across the GitHub platform. Engineering principles Explore best practices for building software at scale with a majority remote team. Infrastructure Get a glimpse at the technology underlying the world’s leading AI-powered developer platform. Platform security Learn how we build security into everything we do across the developer lifecycle. User experience Find out what goes into making GitHub the home for all developers. How we use GitHub to be more productive, collaborative, and secure Our engineering and security teams do some incredible work. Let’s take a look at how we use GitHub to be more productive, build collaboratively, and shift security left. Learn more Enterprise software Back Enterprise software Explore how to write, build, and deploy enterprise software at scale. Automation Automating your way to faster and more secure ships. CI/CD Guides on continuous integration and delivery. Collaboration Tips, tools, and tricks to improve developer collaboration. DevOps DevOps resources for enterprise engineering teams. DevSecOps How to integrate security into the SDLC. Governance & compliance Ensuring your builds stay clean. How enterprise engineering teams can successfully adopt AI Learn how to bring AI to your engineering teams and maximize the value that you get from it. Learn more News & insights Back News & insights Keep up with what’s new and notable from inside GitHub. Company news An inside look at news and product updates from GitHub. Product The latest on GitHub’s platform, products, and tools. Octoverse Insights into the state of open source on GitHub. Policy The latest policy and regulatory changes in software. Research Data-driven insights around the developer ecosystem. The library Older news and updates from GitHub. Unlocking the power of unstructured data with RAG Learn how to use retrieval-augmented generation (RAG) to capture more insights. Learn more Open Source Back Open Source Everything open source on GitHub. Git The latest Git updates. Maintainers Spotlighting open source maintainers. Social impact How open source is driving positive change. Gaming Explore open source games on GitHub. An introduction to innersource Organizations worldwide are incorporating open source methodologies into the way they build and ship their own software. Learn more Security Back Security Stay up to date on everything security. Application security Application security, explained. Supply chain security Demystifying supply chain security. Vulnerability research Updates from the GitHub Security Lab. Web application security Helpful tips on securing web applications. The enterprise guide to AI-powered DevSecOps Learn about core challenges in DevSecOps, and how you can start addressing them with AI and automation. Learn more Changelog Docs Customer stories Contact sales Use Copilot for free Home / News & insights / Research Research: quantifying GitHub Copilot’s impact on developer productivity and happiness When the GitHub Copilot Technical Preview launched just over one year ago, we wanted to know one thing: Is this tool helping developers? The GitHub Next team conducted research using a combination of surveys and experiments, which led us to expected and unexpected answers. Eirini Kalliamvakou·@ikaliam September 7, 2022 | Updated May 21, 2024 | 8 minutes Share: Everyday, we use tools and form habits to achieve more with less. Software development produces such a high number of tools and technologies to make work efficient, to the point of inducing decision fatigue. When we first launched a technical preview of GitHub Copilot in 2021, our hypothesis was that it would improve developer productivity and, in fact, early users shared reports that it did. In the months following its release, we wanted to better understand and measure its effects with quantitative and qualitative research. To do that, we first had to grapple with the question: what does it mean to be productive? Why is developer productivity so difficult to measure? When it comes to measuring developer productivity, there is little consensus and there are far more questions than answers. For example: What are the “right” productivity metrics? [1, 2] How valuable are self-reports of productivity? [3] Is the traditional view of productivity—outputs over inputs—a good fit for the complex problem solving and creativity involved in development work? [4]. In a 2021 study, we found that developers’ own view of productivity has a twist–it’s more akin to having a good day. The ability to stay focused on the task at hand, make meaningful progress, and feel good at the end of a day’s work make a real difference in developers’ satisfaction and productivity. This isn’t a one-off finding, either. Other academic research shows that these outcomes are important for developers [5] and that satisfied developers perform better [6, 7]. Clearly, there’s more to developer productivity than inputs and outputs. How do we think about developer productivity at GitHub? Because AI-assisted development is a relatively new field, as researchers we have little prior research to draw upon. We wanted to measure GitHub Copilot’s effects, but what are they? After early observations and interviews with users, we surveyed more than 2,000 developers to learn at scale about their experience using GitHub Copilot. We designed our research approach with three points in mind: Look at productivity holistically. At GitHub we like to think broadly and sustainably about developer productivity and the many factors that influence it. We used the SPACE productivity framework to pick which aspects to investigate. Include developers’ first-hand perspective. We conducted multiple rounds of research including qualitative (perceptual) and quantitative (observed) data to assemble the full picture. We wanted to verify: (a) Do users’ actual experiences confirm what we infer from telemetry? (b) Does our qualitative feedback generalize to our large user base? Assess GitHub Copilot’s effects in everyday development scenarios. When setting up our studies, we took extra care to recruit professional developers, and to design tests around typical tasks a developer might work through in a given day. Let’s dig in and see what we found! Finding 1: Developer productivity goes beyond speed Through a large-scale survey, we wanted to see if developers using GitHub Copilot see benefits in other areas beyond speeding up tasks. Here’s what stood out: Improving developer satisfaction. Between 60–75% of users reported they feel more fulfilled with their job, feel less frustrated when coding, and are able to focus on more satisfying work when using GitHub Copilot. That’s a win for developers feeling good about what they do! Conserving mental energy. Developers reported that GitHub Copilot helped them stay in the flow (73%) and preserve mental effort during repetitive tasks (87%). That’s developer happiness right there, since we know from previous research that context switches and interruptions can ruin a developer’s day, and that certain types of work are draining [8, 9]. What the charts show, and how to read them 📝 Helpful context: In the survey, we included statements that covered all dimensions of the SPACE framework. SPACE is the acronym for a multidimensional framework that describes several aspects of developer productivity: (1) Satisfaction and well-being; (2) Performance; (3) Activity; (4) Communication and collaboration; and (5) Efficiency and flow. In this post, we focus on the results from the dimensions of “Satisfaction and well-being” and “Efficiency and flow.” Both dimensions are critical to developer productivity and are often overlooked. We’re also preparing more publications of our findings and data, so we can’t share everything today! 📊 What the charts show: The charts below show the percentage of respondents who agreed with the corresponding statement (strongly agree + agree). The >2,000 responses we received came from developers that were signed up in the Technical Preview. They were primarily professional developers (~60%), though we also received responses from students (~30%), and developers who identified as hobbyists (~7%). If you want a refresher on how we also correlated the productivity results to how much developers use GitHub Copilot, check out our previous blog post, and our academic paper. Table: Survey responses measuring dimensions of developer productivity when using GitHub Copilot All questions were modeled off of the SPACE framework. Developers see GitHub Copilot as a productivity aid, but there’s more to it than that. One user described the overall experience: (With Copilot) I have to think less, and when I have to think it’s the fun stuff. It sets off a little spark that makes coding more fun and more efficient. - Senior Software Engineer The takeaway from our qualitative investigation was that letting GitHub Copilot shoulder the boring and repetitive work of development reduced cognitive load. This makes room for developers to enjoy the more meaningful work that requires complex, critical thinking and problem solving, leading to greater happiness and satisfaction. Finding 2: … but speed is important, too In the survey, we saw that developers reported they complete tasks faster when using GitHub Copilot, especially repetitive ones. That was an expected finding (GitHub Copilot writes faster than a human, after all), but >90% agreement was still a pleasant surprise. Developers overwhelmingly perceive that GitHub Copilot is helping them complete tasks faster—can we observe and measure that effect in practice? For that we conducted a controlled experiment. Figure: Summary of the experiment process and results We recruited 95 professional developers, split them randomly into two groups, and timed how long it took them to write an HTTP server in JavaScript. One group used GitHub Copilot to complete the task, and the other one didn’t. We tried to control as many factors as we could–all developers were already familiar with JavaScript, we gave everyone the same instructions, and we leveraged GitHub Classroom to automatically score submissions for correctness and completeness with a test suite. We’re sharing a behind-the-scenes blog post soon about how we set up our experiment! In the experiment, we measured—on average—how successful each group was in completing the task and how long each group took to finish. The group that used GitHub Copilot had a higher rate of completing the task (78%, compared to 70% in the group without Copilot). The striking difference was that developers who used GitHub Copilot completed the task significantly faster–55% faster than the developers who didn’t use GitHub Copilot. Specifically, the developers using GitHub Copilot took on average 1 hour and 11 minutes to complete the task, while the developers who didn’t use GitHub Copilot took on average 2 hours and 41 minutes. These results are statistically significant (P=.0017) and the 95% confidence interval for the percentage speed gain is [21%, 89%]. There’s more to uncover! We’re conducting more experiments and a more thorough analysis of the experiment data we already collected—looking into heterogeneous effects, or potential effects on the quality of code—and we are planning further academic publications to share our findings. What do these findings mean for developers? We’re here to support developers while they build software—that includes working more efficiently and finding more satisfaction in their work. In our research, we saw that GitHub Copilot supports faster completion times, conserves developers’ mental energy, helps them focus on more satisfying work, and ultimately find more fun in the coding they do. We’re also hearing that these benefits are becoming material to engineering leaders in companies that ran early trials with GitHub Copilot. When they consider how to keep their engineers healthy and productive, they are thinking through the same lens of holistic developer wellbeing and promoting the use of tools that bring delight. The engineers’ satisfaction with doing edgy things and us giving them edgy tools is a factor for me. Copilot makes things more exciting. - CTO, Large Engineering Org With the advent of GitHub Copilot, we’re not alone in exploring the impact of AI-powered code completion tools! In the realm of productivity, we recently saw an evaluation with 24 students, and Google’s internal assessment of ML-enhanced code completion. More broadly, the research community is trying to understand GitHub Copilot’s implications in a number of contexts: education, security, labor market, as well as developer practices and behaviors. We are all currently learning by trying GitHub Copilot in a variety of settings. This is an evolving field, and we’re excited for the findings that the research community — including us — will uncover in the months to come. Acknowledgements We are very grateful to all the developers who participated in the survey and experiments–we would be in the dark without your input! GitHub Next conducted the experiment in partnership with the Microsoft Office of the Chief Economist, and specifically in collaboration with Sida Peng and Aadharsh Kannan. Tags: GitHub Copilot research Written by Eirini Kalliamvakou @ikaliam GitHub Copilot research More on GitHub Copilot GitHub for Beginners: Essential features of GitHub Copilot Get the most out of Copilot with code completion, inline chat, slash commands, Copilot code review, and more. Kedasha Kerr Video: How to run dependency audits with GitHub Copilot Learn to automate dependency management using GitHub Copilot, GitHub Actions, and Dependabot to eliminate manual checks, improve security, and save time for what really matters. Andrea Griffiths Related posts Company news GitHub Availability Report: February 2025 In February, we experienced two incidents that resulted in degraded performance across GitHub services. Jakub Oleksy Company news GitHub Availability Report: January 2025 In January, we experienced two incidents that resulted in degraded performance across GitHub services. Jakub Oleksy News & insights GitHub Copilot: The agent awakens Introducing agent mode for GitHub Copilot in VS Code, announcing the general availability of Copilot Edits, and providing a first look at our SWE agent. Thomas Dohmke Explore more from GitHub Docs Everything you need to master GitHub, all in one place. Go to Docs Join GitHub Galaxy Register now for our global enterprise event on March 28–31. Register now GitHub Copilot Don’t fly solo. Try 30 days for free. Learn more Work at GitHub! Check out our current job openings. Apply now We do newsletters, too Discover tips, technical guides, and best practices in our biweekly newsletter just for devs. Your email address Subscribe Yes please, I’d like GitHub and affiliates to use my information for personalized communications, targeted advertising and campaign effectiveness. See the GitHub Privacy Statement for more details. Subscribe Product Features Security Enterprise Customer Stories Pricing Resources Platform Developer API Partners Atom Electron GitHub Desktop Support Docs Community Forum Training Status Contact Company About Blog Careers Press Shop LinkedIn icon GitHub on LinkedIn Instagram icon GitHub on Instagram YouTube icon GitHub on YouTube X icon GitHub on X TikTok icon GitHub on TikTok Twitch icon GitHub on Twitch GitHub icon GitHub’s organization on GitHub © 2025 GitHub, Inc. Terms Privacy Manage Cookies Do not share my personal information

---


# TITLE: Cursor vs GitHub Copilot: Which AI Coding Assistant is better?
# URL: https://www.builder.io/blog/cursor-vs-github-copilot
# CONTENT:
Cursor vs GitHub Copilot: Which AI Coding Assistant is better? Skip to main content Windsurf vs Cursor: which is the better AI code editor? Announcing Visual Copilot - Figma to production in half the time builder.io Visual Development Platform Platform Overview Learn about our AI capabilities Develop﻿ Convert designs to clean code Leverage your components Use your framework Publish Visually edit Manage headless content Optimize experiences Integrate with your stack Platform Use Cases Headless Commerce Design to Code Marketing Sites Landing Pages Mobile Apps Multi-Brand Integrations Vercel Netlify Shopify Cloudinary Salesforce Figma Algolia Phrase See All Solutions Overview Documentation Devtools Builder Blueprints Frameworks React Next.js Qwik Gatsby Angular Vue Svelte Remix Nuxt Astro See All Open Source Builder 8k Mitosis 13k AI Shell 5k Micro Agent 4k GPT Crawler 21k Qwik 21k Partytown 13k Explore Best of Web Performance Insights Developers Pricing Explore Blog Product Updates Glossary Community Forum Developer Docs Knowledge Base Partners Templates Customers Success Stories Showcase Resource Center Product Demos Guides Webinars Explainers See All Resources Contact Sales Go to App Windsurf vs Cursor: which is the better AI code editor? Announcing Visual Copilot - Figma to production in half the time builder.io Blog Home Resources Blog Forum Github Login Signup × Visual CMS Drag-and-drop visual editor and headless CMS for any tech stack Theme Studio for Shopify Build and optimize your Shopify-hosted storefront, no coding required Resources Blog Get Started Login ☰ ‹ Back to blog AI Cursor vs GitHub Copilot December 20, 2024 Written By Vishwas Gopinath Let's talk about AI coding assistants. They're basically the new hot topic in dev tools, and for good reason. They're designed to help developers write code more efficiently and with fewer errors. They're pretty dang cool. While there are quite a few players in this space, let's zoom in on two of the major contenders. In one corner, we've got GitHub Copilot, the established player. In the other, Cursor, the new kid on the block shaking things up. Both are trying to make our lives easier as devs. After spending some time with Copilot and Cursor, I thought it might be useful to break down how they stack up against each other. Whether you're curious about trying them out or just want to know what all the fuss is about, stick around. We're going to dig into some of the key features that matter to us as developers. Cursor AI Cursor is basically an AI-powered code editor on steroids. It is a fork of Visual Studio Code, bringing advanced AI capabilities to a familiar interface. GitHub Copilot GitHub Copilot is an AI coding assistant that helps you write code faster and with less effort, allowing you to focus more energy on problem solving and collaboration. Developed by GitHub in collaboration with OpenAI, it uses machine learning to generate code suggestions and complete tasks directly in your code editor (Visual Studio Code, Visual Studio, Vim/Neovim, JetBrains IDEs). Both Cursor AI and GitHub Copilot offer a range of features designed to enhance productivity and support developers. Let's dive into the specific features and see how these two compare. Tab completion You know that feeling when you're typing and the IDE just gets what you're trying to do? That's what we're talking about here. Cursor Cursor's tab completion is pretty wild. It'll suggest multiple lines of code, and it's looking at your whole project to make those suggestions. For TypeScript and Python files - when Tab suggests an unimported symbol, Cursor will auto-import it to your current file. Plus, it even tries to guess where you're going to edit next. Copilot Copilot's more focused on inline suggestions. Tab to accept, and you're off to the races. It often predicts the next logical line of code based on the developer's style, which can significantly speed up coding tasks. Need options? Hit Alt+] or Alt+[ to cycle through different suggestions, or Ctrl+Enter to see multiple alternatives in a new tab. Code generation This is where things get interesting. Imagine describing what you want your code to do, and boom — it's there. Cursor Cursor has this thing called Composer that can create entire applications based on your description. It's looking at your whole project when it generates code, so it tries to match your style. Use ⌘ + I to open it, and ⌘ + N to create a new Composer. For inline generation, boilerplate code and edits, you can use the ⌘ + K feature. Impressively, it can work with multiple programming languages within the same project, adapting its suggestions accordingly. Copilot Copilot's more about those inline suggestions, but Copilot Chat can handle bigger chunks of code if you ask it nicely. The CLI can also generate code if you describe what you want in plain English. Chat Sometimes you just need to ask a question. But is chatting with an AI actually helpful? Cursor Cursor's chat (⌘ + L) is context-aware, so it knows what you're working on. You can also drag & drop folders into Chat to provide additional context and apply code suggestions right from the chat, which is neat. It even supports images for visual context. Copilot GitHub Copilot Chat is similar — you can ask it to explain code or suggest improvements. It's integrated right into VS Code, so it feels pretty seamless. They've been rolling out some new features lately, like better chat history, drag and & folders and ways to attach more context. But if you're already using Cursor, you might not find anything groundbreaking here. Terminal Terminal work can be a pain, especially with complex commands. Cursor Cursor extends its AI smarts to the terminal with ⌘ + K. It's pretty handy for translating vague ideas into actual commands. However, it hijacks the terminal's clear shortcut, which is just kind of annoying. Copilot Copilot's got a slick terminal integration that lets you just hit ⌘ + I, type what you want, and get the command you need. No need to be a bash wizard anymore – just describe what you want to do in plain English, hit enter, and you're good to go. The command + enter shortcut to run the suggested command is very handy. Context awareness This is a big one. Can these tools actually understand your whole project, or are they just looking at the current file? Cursor Cursor's pretty impressive here. It looks at your entire codebase and project structure. You can even use @ symbols to reference specific parts of your project, like @Files, @Folders, @Code, and more. Copilot Copilot's pretty smart about context too. It looks at your open files to figure out what's going on and can pick up cues from your imports, comments, and function names. Use a '#' to reference files or use the 'Attach Context' button to pick exactly what you want Copilot to look at. Multi-file support Support for working across multiple files enables AI assistants to understand and modify complex project structures. Cursor Cursor's Composer can make changes across your entire project, which is pretty powerful. It understands how different files and components relate to each other. Composer can generate files for an entire app at once. I’ve personally used the feature to refactor a single file into more organized files and folders. Copilot Copilot's latest addition is its Edits feature, and it's pretty sweet. Just define your working set of files, describe what you want in plain English, and watch Copilot make changes across multiple files. You can review each change, accept what works, and iterate until you get it right. In our testing, we found the feature to be surprisingly slow, sometimes getting stuck in infinite loading states or making incorrect file changes. Pro tip: manually specify your files instead of relying on automatic detection – it's more work, but you'll get more reliable results. AI agent This is about having an AI assistant that can take control of your editor – running commands, managing files, and handling project-wide tasks. Cursor In Composer, hit ⌘. and you've got yourself Cursor Agent, a super-powered AI assistant. It'll automatically grab context, run terminal commands, handle files, and even do semantic code search. The catch? It only works with Claude models, and each operation counts against your quota. But when you need to get things done fast, it's an absolute game-changer. Copilot Nothing quite like this in Copilot's toolkit yet. While Copilot Chat can handle some similar tasks, it doesn't have the same level of integrated project-wide assistance. Converting Figma designs to code This is about compatibility with other AI tools like Visual Copilot. Cursor To convert Figma designs to code, simply launch the Figma plugin, select the element or frame you want to convert, and click Export to Code. Copy the resulting command and paste it into the Cursor terminal. From there, you can instruct Cursor to add interactivity, animations, event handlers and other enhancements. It will generate all the necessary code based on your requirements. Copilot VS Code with Copilot also supports design-to-code conversion. Simply paste the command in the terminal and instruct Copilot to enhance the generated code according to your requirements. Code review We all need a second pair of eyes sometimes. AI-powered code review can provide automated feedback on code quality, potential bugs, and adherence to best practices. Cursor Cursor's new bug finder is pretty neat. It scans your code and branch changes against main, rating each potential bug it finds. One click and it'll fix things right in your editor. There's a catch though – you'll pay per click (we're talking a dollar or more each time) Copilot Copilot's just rolled out a code review feature that's pretty sweet (though still in limited release). Hit the review button in your Source Control tab, and it'll check your staged or unstaged changes. It drops suggestions right inline in your code that you can apply with one click. Skip what you don't like, accept what you do – simple as that. Customization One size doesn't fit all in coding. Can you bend these tools to fit your specific needs, or are you stuck with what they give you? Cursor Cursor lets you set custom instructions through settings and .cursorrules files. You can tailor it to your project's specific needs. Copilot Copilot supports custom instructions through a .github/copilot-instructions.md file, similar to Cursor's approach. This lets you specify your coding preferences and conventions, which Copilot will follow when generating code. AI commit messages Let's see how these tools handle git commits. Cursor AI-generated commit messages might not sound like much, but it saves me a couple of minutes every day and reduces the mental load of coming up with good commit messages. Now, Cursor does have this habit of being a bit...wordy with its commit messages. You can tweak that behavior by adding some instructions in the .cursorrules file. Copilot Copilot does this pretty nice by default. Just hit the auto-generate commit message button and you're good to go. I've found their messages to be cleaner and more concise compared to Cursor's, though you'll probably still want to give them a quick review before committing. IDE integration Nobody wants to switch between a bunch of tools. Cursor Cursor is its own thing — it's built on top of VS Code, so it'll feel familiar if you're used to that. Copilot Copilot on the other hand integrates with various IDEs — VS Code, IntelliJ, Neovim. The CLI works in any terminal. Models Cursor Cursor offers a range of models, including GPT-4o, o1, Claude 3.5 Sonnet, and their custom cursor-small model. You can choose based on what you need — speed or capability. Copilot Copilot has expanded its model offerings significantly. You can now choose between different models including Claude 3.5 Sonnet, o1, and GPT-4o. This flexibility lets you optimize for different tasks – whether you need quick code completions or more complex reasoning and understanding. Pricing Let's talk money. How do their pricing models compare? Cursor Cursor has a free Hobby tier with limited features, a Pro tier for $20/month, and a Business tier for $40/user/month. Copilot Copilot now offers a free tier with limited features (like 12,000 completions per month), while Pro plans start at $10/month. For teams, there's Business at $19/user/month and Enterprise at $39/user/month. Wrapping up: and the winner is... After diving deep into both Cursor and GitHub Copilot, it's time to pick a champ. And drum roll, please...Cursor takes the crown. Its unique features make it a hard sell to beat in the world of AI-assisted coding. Now, don't get me wrong, Copilot is a solid tool. It's great for quick suggestions and it plays nice with a bunch of different IDEs. But Cursor? It's just operating on another level. Here's why Cursor wins out: Project-wide smarts: Cursor's ability to understand and work with your entire codebase is still unmatched. Copilot's context awareness can get sluggish with larger projects. Speed and reliability: Cursor's Composer consistently outperforms when it comes to project-wide operations. While Copilot's new Edits feature is promising, it often gets stuck or slows to a crawl. Cursor just gets the job done. Agent power: With Cursor, you get a polished Agent mode that just gets what you're trying to do Copilot is still a powerful tool, and if you're looking for something that's easy to integrate into your existing workflow, it might be the way to go. Its code completion capabilities are certainly impressive. But if you want to really push the boundaries of what AI can do for your coding, Cursor is where it's at. My preferred AI stack My preferred workflow, overall look like this: For coding, I work iteratively with Cursor Our design team works within Figma Builder.io converts designs to code and also patches in design updates as they're needed But those are just my preferences, what are yours? If you enjoyed this post, you might also like: $500 Devin vs $20 Cursor Windsurf vs Cursor Training Your Own AI Model Introducing Visual Copilot: convert Figma designs to high quality code in a single click. Try Visual Copilot Get a demo Share Introducing Visual Copilot: A new AI model to turn Figma designs to high quality code using your components. Try Visual Copilot Get a demo Don't want to miss our articles? Join Our Newsletter Introducing Visual Copilot: A new AI model to turn Figma designs to high quality code using your components. Try Visual Copilot Get a demo Like our content? Join Our Newsletter Continue Reading AI 15 MIN What is an LLM (For Web Developers) March 13, 2025 AI14 MIN How I use Cursor (+ my best tips) March 11, 2025 Design to Code5 MIN Export Figma to CSS March 10, 2025 Product Visual CMS Theme Studio for Shopify Sign up Login Featured Integrations React Angular Next.js Gatsby Resources User Guides Developer Docs Forum Blog Github Get In Touch Chat With Us Twitter Linkedin Careers © 2020 Builder.io, Inc. Security Privacy Policy Terms of Service Get the latest from Builder.io Dev Drop Newsletter News, tips, and tricks from Builder, for frontend developers. Product Newsletter Latest features and updates on the Builder.io platform By submitting, you agree to our Privacy Policy Product Platform Overview Integrations What's New CAPABILITIES Design to Code Visual Editor Headless CMS A/B & Personalization Company About Careers Developers Documentation Devtools Builder Blueprints Best of Web Performance Insights Open Source Builder Mitosis AI Shell Micro Agent GPT Crawler Qwik Partytown Solutions Headless Commerce Marketing Sites Landing Pages Mobile Apps Multi-brand Popular Guides Figma to Code Guide Composable Commerce Guide Headless CMS Guide Headless Commerce Guide Composable DXP Guide Design to Code Resources Blog Knowledge Base Community Forum Partners Templates Success Stories Showcase Resource Center Glossary Page Builder > Frameworks React Next.js Qwik Gatsby Angular Vue Svelte Remix Nuxt Astro See All © 2025 Builder.io, Inc. Security Privacy Policy SaaS Terms Security & Compliance Cookie Preferences

---


# TITLE: GitHub Copilot, Cursor, or Windsurf: A Developer’s Guide to AI IDEs
# URL: https://medium.com/@gvelosa/github-copilot-cursor-or-windsurf-a-developers-guide-to-ai-ides-e69181a6f75f
# CONTENT:
Copilot vs. Cursor vs. Windsurf: Which AI Coding IDE is Right for You? | by Gonçalo Velosa | Jan, 2025 | Medium Open in app Sign up Sign in Write Sign up Sign in Home Library Stories Stats Copilot vs. Cursor vs. Windsurf: Which AI Coding IDE is Right for You? Gonçalo Velosa · Follow 4 min read · Jan 31, 2025 -- Listen Share The AI revolution is in full swing, and it’s changing the way we write code faster than we can say “segmentation fault.” We’ve moved beyond simple code completion to having full-fledged AI companions that can understand our code, suggest improvements, and even write entire blocks of logic for us. As seasoned developers, the question is no longer if we should embrace these tools, but which one best suits our needs. Previously, we looked at GitHub Copilot and Cursor AI. Now, there is a new contender in the arena: Windsurf IDE. Let’s dive into this three-way battle and see how these AI coding powerhouses stack up against each other. A Quick Flyby: Copilot, Cursor, and Now Windsurf Let’s refresh our memory and introduce our new challenger: GitHub Copilot: The veteran, developed by GitHub and OpenAI, trained on a massive dataset of public GitHub repositories. It offers strong code completion, broad language support, and now boasts Copilot Chat, an in-editor AI assistant. Cursor AI: An AI-first IDE, Cursor is built from the ground-up for AI-powered coding. It excels at refactoring, understanding your codebase, and providing a natural language interface for code editing. It also features a robust built-in AI chat. Windsurf IDE: The newcomer, positioned as a faster, more web-focused AI coding tool. It’s built with a local-first approach, meaning all AI computations can be performed on your own device, ensuring privacy and speed. Like Cursor, it’s an IDE designed specifically for AI development. Feature Breakdown: Strengths and Weaknesses Let’s dissect each tool’s strengths, focusing on what matters most to us as developers: GitHub Copilot: Strengths: Vast Training Data: Unmatched breadth of code examples it’s learned from. Excellent Code Completion: Especially for common patterns and boilerplate. Wide Language Support: Covers virtually every popular language. Copilot Chat: A powerful addition for getting code explanations and generating code from natural language. Mature and Widely Adopted: Large community, extensive documentation. Weaknesses: Limited Codebase Understanding: Primarily focuses on the current file, not the entire project. Basic Refactoring: Not its strongest suit. Cursor AI: Strengths: Superior Refactoring: Helps optimize and improve existing code. Deep Codebase Knowledge: Learns your project’s structure for tailored suggestions. Natural Language Editing: Modify code with simple instructions. Built-in AI Chat: Seamlessly integrated into the coding workflow. Weaknesses: Smaller Training Data: Compared to Copilot. Higher Price Point: More expensive than Copilot. Windsurf IDE: Strengths: Local-First AI (Optional): Offers the ability to run the AI models locally, enhancing privacy and potentially speed if you have a powerful machine with a dedicated GPU. Web-Focused: Built with web technologies in mind, potentially leading to smoother performance for web development. Fast Performance: Claims to be faster than competitors in terms of response time. Built-in AI Chat: Facilitates a more interactive coding experience. Weaknesses: Newer Entrant: Smaller community, potentially fewer resources. Less Mature: May have some rough edges compared to more established tools. Requires High-End Hardware for Optimal Local Performance: To get the most out of local AI, you’ll need a machine with a powerful GPU. Codebase Understanding: According to the article shared, the codebase understanding is not as good as Cursor. The Price of Progress: Comparing Costs GitHub Copilot: $10/month or $100/year for individuals. Includes Copilot Chat. Cursor AI: $20/month for the basic plan (for individuals). Windsurf IDE: Offers a free tier with limited features. The Pro plan (for individuals) is $15/month. Capabilities Compared: A Three-Way Showdown The Verdict: Which AI Co-pilot Should You Choose? The “best” tool depends on your specific needs and priorities. Here’s a breakdown to help you decide: Choose GitHub Copilot if: You want a reliable, widely-used tool with excellent code completion. You need support for a vast range of programming languages. You want a cost-effective solution that includes powerful chat features. You value a large community and extensive resources. Choose Cursor AI if: Refactoring and improving existing code is a top priority. You want an AI that deeply understands your codebase. You prefer a natural language interface for code editing. You’re willing to pay a premium for advanced features. Choose Windsurf IDE if: Data privacy is paramount, and you want the option to run AI models locally. You primarily work on web projects. You value raw speed and responsiveness and have a powerful GPU. You’re willing to try a newer, less mature tool with the potential for rapid development. Final Thoughts: The AI coding landscape is evolving at breakneck speed. Copilot, Cursor, and Windsurf each offer a unique take on AI-assisted development. I strongly encourage you to take advantage of their free trials (or free tiers) to experiment and see which one best fits your workflow. With these powerful tools at our disposal, we can spend less time on mundane tasks and more time on the creative, problem-solving aspects that make software development so engaging. The future of coding is here, and it’s intelligent! Artificial Intelligence Ai Pair Programmer Productivity Developer Ai Development -- -- Follow Written by Gonçalo Velosa 3 Followers ·82 Following Blockchain engineer, avid gamer, soon-to-be dad. I delve into code, virtual worlds, & soon, parenthood adventures. Follow No responses yet Help Status About Careers Press Blog Privacy Terms Text to speech Teams

---


# TITLE: Measuring GitHub Copilot’s Impact on Productivity – Communications of the ACM
# URL: https://cacm.acm.org/research/measuring-github-copilots-impact-on-productivity/
# CONTENT:
Measuring GitHub Copilot’s Impact on Productivity – Communications of the ACM Skip to content Explore Topics Architecture and Hardware Artificial Intelligence and Machine Learning Computer History Computing Applications Computing Profession Data and Information Education HCI Philosophy of Computing Security and Privacy Society Software Engineering and Programming Languages Systems and Networking Theory Latest Issue Latest Issue March 2025, Vol. 68 No. 3 Previous Issue February 2025, Vol. 68 No. 2 Explore the archive Search Open Membership Navigation Settings Sign Out Sign In Join ACM Topics Architecture and Hardware Artificial Intelligence and Machine Learning Computer History Computing Applications Computing Profession Data and Information Education HCI Philosophy of Computing Security and Privacy Society Software Engineering and Programming Languages Systems and Networking Theory Sections Research and Advances Opinion Practice News Research Highlights Careers Magazine Latest Issue Magazine Archive Editorial Staff and Board Submit an Article Alerts & Feeds Author Guidelines CACM Web Account Membership in ACM includes a subscription to Communications of the ACM (CACM), the computing industry's most trusted source for staying connected to the world of advanced computing. Sign In Sign Up Communications of the ACM About Us Frequently Asked Questions Contact Us Follow Us CACM on Twitter CACM on Reddit CACM on LinkedIn Research and Advances Computing Profession Measuring GitHub Copilot’s Impact on Productivity A case study asks Copilot users about the tool's impact on their productivity, and seeks to find their perceptions mirrored in user data. By Albert Ziegler, Eirini Kalliamvakou, X. Alice Li, Andrew Rice, Devon Rifkin, Shawn Simister, Ganesh Sittampalam, and Edward Aftandilian Posted Feb 15 2024 Share Twitter Reddit Hacker News Print Join the Discussion View in the ACM Digital Library Background Data and Methodology What Drives Perceived Productivity? Experience Variation over Time Conclusions Code-completion systems offering suggestions to a developer in their integrated development environment (IDE) have become the most frequently used kind of programmer assistance.1 When generating whole snippets of code, they typically use a large language model (LLM) to predict what the user might type next (the completion) from the context of what they are working on at the moment (the prompt).2 This system allows for completions at any position in the code, often spanning multiple lines at once. Key Insights AI pair-programming tools such as GitHub Copilot have a big impact on developer productivity. This holds for developers of all skill levels, with junior developers seeing the largest gains. The reported benefits of receiving AI suggestions while coding span the full range of typically investigated aspects of productivity, such as task time, product quality, cognitive load, enjoyment, and learning. Perceived productivity gains are reflected in objective measurements of developer activity. While suggestion correctness is important, the driving factor for these improvements appears to be not correctness as such, but whether the suggestions are useful as a starting point for further development. Potential benefits of generating large sections of code automatically are huge, but evaluating these systems is challenging. Offline evaluation, where the system is shown a partial snippet of code and then asked to complete it, is difficult not least because for longer completions there are many acceptable alternatives and no straightforward mechanism for labeling them automatically.5 An additional step taken by some researchers3,21,29 is to use online evaluation and track the frequency of real users accepting suggestions, assuming that the more contributions a system makes to the developer’s code, the higher its benefit. The validity of this assumption is not obvious when considering issues such as whether two short completions are more valuable than one long one, or whether reviewing suggestions can be detrimental to programming flow. Code completion in IDEs using language models was first proposed in Hindle et al.,9 and today neural synthesis tools such as GitHub Copilot, CodeWhisperer, and TabNine suggest code snippets within an IDE with the explicitly stated intention to increase a user’s productivity. Developer productivity has many aspects, and a recent study has shown that tools like these are helpful in ways that are only partially reflected by measures such as completion times for standardized tasks.23,a Alternatively, we can leverage the developers themselves as expert assessors of their own productivity. This meshes well with current thinking in software engineering research suggesting measuring productivity on multiple dimensions and using self-reported data.6 Thus we focus on studying perceived productivity. Here, we investigate whether usage measurements of developer interactions with GitHub Copilot can predict perceived productivity as reported by developers. We analyze 2,631 survey responses from developers using GitHub Copilot and match their responses to measurements collected from the IDE. We consider acceptance counts and more detailed measures of contribution, such as the amount of code contributed by GitHub Copilot and persistence of accepted completions in the code. We find that acceptance rate of shown suggestions is a better predictor of perceived productivity than the alternative measures. We also find that acceptance rate varies significantly over our developer population as well as over time, and present a deeper dive into some of these variations. Our results support the principle that acceptance rate can be used for coarse-grained monitoring of the performance of a neural code synthesis system. This ratio of shown suggestions being accepted correlates better than more detailed measures of contribution. However, other approaches remain necessary for fine-grained investigation due to the many human factors involved. Background Offline evaluation of code completion can have shortcomings even in tractable circumstances where completions can be labeled for correctness. For example, a study of 15,000 completions by 66 developers in Visual Studio found significant differences between synthetic benchmarks used for model evaluation and real-world usage.7 The evaluation of context-aware API completion for Visual Studio IntelliCode considered Recall@5—the proportion of completions for which the correct method call was in the top five suggestions. This metric fell from 90% in offline evaluation to 70% when used online.21 Figure 1. GitHub Copilot’s code completion funnel. Offline evaluation of code completion can have shortcomings even in tractable circumstances. Due to the diversity of potential solutions to a multi-line completion task, researchers have used software testing to evaluate the behavior of completions. Competitive programming sites have been used as a source of such data8,11 as well as handwritten programming problems.5 Yet, it is unclear how well performance on programming competition data generalizes to interactive development in an IDE. It is unclear how well performance on programming competition data generalizes to interactive development in an IDE. In this work, we define acceptance rate as the fraction of completions shown to the developer that are subsequently accepted for inclusion in the source file. The IntelliCode Compose system uses the term click through rate (CTR) for this and reports a value of 10% in online trials.20 An alternative measure is that of daily completions accepted per user (DCPU) for which a value of around 20 has been reported.3,29 To calculate acceptance rate one must, of course, normalize DCPU by the time spent coding each day. For context, in our study GitHub Copilot has an acceptance rate of 27% and a mean DCPU in excess of 312 (See Figure 1).b These differences are presumably due to differences in the kinds of completion offered, or perhaps to user-interface choices. We discuss later how developer objectives, choice of programming language, and even time of day seem to affect our data. Such discrepancies highlight the difficulty in using acceptance rate to understand the value of a system. There is some evidence that acceptance rate (and indeed correctness) might not tell the whole story. One survey of developers considered the use of AI to support translation between programming languages and found indications that developers tolerated, and in some cases valued, erroneous suggestions from the model.26 There is some evidence that acceptance rate (and indeed correctness) might not tell the whole story. Measuring developer productivity through activity counts over time (a typical definition of productivity borrowed from economics) disregards the complexity of software development as they account for only a subset of developer outputs. A more holistic picture is formed by measuring perceived productivity through self-reported data across various dimensions6 and supplementing it with automatically measured data.4 We used the SPACE framework6 to design a survey that captures self-reported productivity and paired the self-reported data with usage telemetry. To the best of our knowledge, this is the first study of code suggestion tools establishing a clear link between usage measurements and developer productivity or happiness. A previous study comparing GitHub Copilot against IntelliCode with 25 participants found no significant correlation between task completion times and survey responses.22 Figure 2. Demographic composition of survey respondents. Data and Methodology Usage measurements. GitHub Copilot provides code completions using OpenAI language models. It runs within the IDE and at appropriate points sends a completion request to a cloud-hosted instance of the neural model. GitHub Copilot can generate completions at arbitrary points in code rather than, for example, only being triggered when a developer types a period for invoking a method on an object. A variety of rules determine when to request a completion, when to abandon requests if the developer has moved on before the model is ready with a completion, and how much of the response from the model to surface as a completion. As stated in our terms of usage,c the GitHub Copilot IDE extension records the events shown in Table 1 for all users. We make usage measurements for each developer by counting those events. Table 1. Developer usage events collected by GitHub Copilot. opportunity A heuristic-based determination by the IDE and the plug-in that a completion might be appropriate at this point in the code (for example, the cursor is not in the middle of a word) shown Completion shown to the developer accepted Completion accepted by the developer for inclusion in the source file accepted_char The number of characters in an accepted completion mostly_unchanged_X Completion persisting in source code with limited modifications (Levenshtein distance less than 33%) after X seconds, where we consider durations of 30, 120, 300, and 600 seconds unchanged_X Completion persisting in source code unmodified after X seconds. (active) hour An hour during which the developer was using their IDE with the plug-in active Our measures of persistence go further than existing work, which stops at acceptance. The intuition here is that a completion which is accepted into the source file but then subsequently turns out to be incorrect can be considered to have wasted developer time both in reviewing it and then having to go back and delete it. We also record mostly unchanged completions: A large completion requiring a few edits might still be a positive contribution. It is not clear how long after acceptance one should confirm persistence, so we consider a range of options. The events pertaining to completions form a funnel which we show quantitatively in Table 1. We include a summary of all data in Appendix A.d (All appendices for this article can be found online at https://dl.acm.org/doi/10.1145/3633453). We normalize these measures against each other and write X_per_Y to indicate we have normalized metric X by metric Y. For example: accepted_per_hour is calculated as the total number of accepted events divided by the total number of (active) hour events. Table 2 defines the core set of metrics which we feel have a natural interpretation in this context. We note that there are other alternatives and we incorporate these in our discussion where relevant. Table 2. The core set of measurements considered in this paper. Natural name Explanation Shown rate Ratio of completion opportunities that resulted in a completion being shown to the user Acceptance rate Ratio of shown completions accepted by the user Persistence rate Ratio of accepted completions unchanged after 30, 120, 300, and 600 seconds Fuzzy persistence rate Ratio of accepted completions mostly unchanged after 30, 120, 300, and 600 seconds Efficiency Ratio of completion opportunities that resulted in a completion accepted and unchanged after 30, 120, 300, and 600 seconds Contribution speed Number of characters in accepted completions per distinct, active hour Acceptance frequency Number of accepted completions per distinct, active hour Persistence frequency Number of unchanged completions per distinct, active hour Total volume Total number of completions shown to the user Loquaciousness Number of shown completions per distinct, active hour Eagerness Number of shown completions per opportunity Productivity survey. To understand users’ experience with GitHub Copilot, we emailed a link to an online survey to 17,420 users. These were participants of the unpaid technical preview using GitHub Copilot with their everyday programming tasks. The only selection criterion was having previously opted in to receive communications. A vast majority of survey users (more than 80%) filled out the survey within the first two days, on or before February 12, 2022. We therefore focus on data from the four-week period leading up to this point (“the study period”). We received a total of 2,047 responses we could match to usage data from the study period, the earliest on Feb. 10, 2022 and the latest on Mar. 6, 2022. The survey contained multiple-choice questions regarding demographic information (see Figure 2) and Likert-style questions about different aspects of productivity, which were randomized in their order of appearance to the user. Figure 2 shows the demographic composition of our respondents. We note the significant proportion of professional programmers who responded. The SPACE framework6 defines 5 dimensions of productivity: Satisfaction and well-being, Performance, Activity, Communication and collaboration, and Efficiency and flow. We use four of these (S,P,C,E) since self reporting on (A) is generally considered inferior to direct measurement. We included 11 statements covering these four dimensions in addition to a single statement: “I am more productive when using GitHub Copilot.” For each self-reported productivity measure, we encoded its five ordinal response values to numeric labels (1 = Strongly Disagree, …, 5 = Strongly Agree). We include the full list of questions and their coding to the SPACE framework in Appendix C. For more information on the SPACE framework and how the empirical software engineering community has been discussing developer productivity, please see the following section. Early in our analysis, we found that the usage metrics we describe in the Usage Measurements section corresponded similarly to each of the measured dimensions of productivity, and in turn these dimensions were highly correlated to each other (Figure 3). We therefore added an aggregate productivity score calculated as the mean of all 12 individual measures (excluding skipped questions). This serves as a rough proxy for the much more complex concept of productivity, facilitating recognition of overall trends, which may be less discernible on individual variables due to higher statistical variation. Figure 3. Correlation between metrics. Metrics are ordered by similarity based on distance in the correlation matrix, except for manually fixing the aggregate productivity and acceptance rate at the end for visibility. The full dataset of these aggregate productivity scores together with the usage measurements considered in this article is available at https://github.com/wunderalbert/prod-neural-materials. Given it has been impossible to produce a unified definition or metric(s) for developer productivity, there have been attempts to synthesize the factors that impact productivity to describe it holistically, include various relevant factors, and treat developer productivity as a composite measure17,19,24 In addition, organizations often use their own multidimensional frameworks to operationalize productivity, which reflects their engineering goals—for example, Google uses the QUANTS framework, with 5 components of productivity.27 In this article, we use the SPACE framework,6 which builds on synthesis of extensive and diverse literature by expert researchers and practitioners in the area of developer productivity. SPACE is an acronym of the five dimensions of productivity: S (Satisfaction and well being): This dimension is meant to reflect developers’ fulfillment with the work they do and the tools they use, as well as how healthy and happy they are with the work they do. This dimension reflects some of the easy-to-overlook trade-offs involved when looking exclusively at velocity acceleration (for example, when we target faster turnaround of code reviews without considering workload impact or burnout for developers). P (Performance): This dimension aims to quantify outcomes rather than output. Example metrics that capture performance relate to quality and reliability, as well as further-removed metrics such as customer adoption or satisfaction. A (Activity): This is the count of outputs—for example, the number of pull requests closed by a developer. As a result this is a dimension that is best quantified via system data. Given the variety of developers’ activities as part of their work, it is important that the activity dimension accounts for more than coding activity—for instance, writing documentation, creating design specs, and so on. C (Communication and collaboration): This dimension aims to capture that modern software development happens in teams and is, therefore, impacted by the discoverability of documentation or the speed of answering questions, or the onboarding time and process of new team members. E (Efficiency and flow): This dimension reflects the ability to complete work or make progress with little interruption or delay. It is important to note that delays and interruptions can be caused either by systems or humans, and it is best to monitor both self-reported and observed measurements—for example, use self-reports of the ability to do uninterrupted work, as well as measure wait time in engineering systems). Developer Productivity and the SPACE Framework Developer productivity has been a controversial topic in software engineering research over the years. We point readers to excellent presentations of the existing discourse in the community in Meyer et al.12 and Murphy-Hill et al.15; however we summarize the key points of discussion below: Inspired by economics definitions of productivity as output per unit of input, some research has defined developer productivity in the same terms—for example, numbers of lines of code per day, function points per sprint, and so on. However, such measures are not connected to goals (for instance, it is not the goal of a developer to write the most lines of code), they may motivate developers to game the system, they do not account for the quality of the output, and they are in tension with other metrics (for example, a higher number of commits or PRs will create a higher need for code reviews). Observational studies of developers reveal that developers spent more than half their working day on activities other than coding.13 Given this, the view of developer productivity as inputs and outputs, or using metrics that strictly focus on coding, ignores the reality of the work developers do. In addition, developers’ perspective on what affects their productivity12 and what metrics might reflect it14 differs from the inputs/outputs view. When asked when they are productive and how they measure productivity, developers do not cite lines of code or function points per sprint, but rather completing tasks, being free of interruptions, usefulness of their work, success of the feature they worked on, and more. To sum up, after many studies and many definitions, measurements, and approaches to productivity, the empirical software engineering research community has concluded that developer productivity is a multidimensional topic that cannot be summarized by a single metric.10 Both objective and subjective approaches to measurement have been tried, leading to the conclusion that they both have advantages and disadvantages. What Drives Perceived Productivity? To examine the relationship between objective measurements of user behavior and self-reported perceptions of productivity, we used our set of core usage measurements (Table 2). We then calculated Pearson’s R correlation coefficient and the corresponding p-value of the F-statistic between each pair of usage measurement and perceived productivity metric. We also computed a PLS regression from all usage measurements jointly. We summarize these results in Figure 3, showing the correlation coefficients between all measures and survey questions. The full table of all results is included in Appendix B, available online. We find acceptance rate (accepted_per_shown) most positively predicts users’ perception of productivity, although, given the confounding and human factors, there is still notable unexplained variance. Of all usage measurements, acceptance rate correlates best with aggregate productivity ( ρ=0.24, P<0.0001). This measurement is also the best performing for at least one survey question in each of the SPACE dimensions. This correlation is high confidence but leaves considerable unexplained variance. Later, we explore improvements from combining multiple usage measurements together. Looking at the more detailed metrics around persistence, we see that it is generally better over shorter time periods than over longer periods. This is intuitive in the sense that shorter periods move the measure closer to acceptance rate. We also expect that at some point after accepting the completion it becomes simply part of the code and so any changes (or not) after that point will not be attributed to GitHub Copilot. All persistence measures were less well correlated than acceptance rate. To assess the different metrics in a single model, we ran a regression using projection on latent structures (PLS). The choice of PLS, which captures the common variation of these variables as is linearly connected to the aggregate productivity,28 is due to the high collinearity of the single metrics. The first component, to which every metric under consideration contributes positively, explains 43.2% of the variance. The second component captures the acceptance rate/change rate dichotomy; it explains a further 13.1%. Both draw most strongly from acceptance rate. This strongly points to acceptance rate being the most immediate indicator of perceived productivity, although it is beneficial to combine with others to get a fuller picture. Experience To understand how different types of developers interact with Copilot, our survey asked respondents to self-report their level of experience in two ways: “Think of the language you have used the most with Copilot. How proficient are you in that language?” with options “Beginner”, “Intermediate”, and “Advanced”. “Which best describes your programming experience?” with options starting with “Student” and ranging from “0-2 years” to “16+ years” in two year intervals. We compute correlations with productivity metrics for both experience variables and include these two variables as covariates in a multivariate regression analysis. We find that both are negatively correlated with our aggregate productivity measure (proficiency: ρ=–0.095, P=0.0001; years of experience: ρ=–0.161, P<0.0001). However, in multivariate regressions predicting productivity from usage metrics while controlling for demographics, proficiency had a non-significant positive effect ( coeff=0.021, P=0.213), while years of experience had a non-significant negative effect ( coeff=–0.032, P=0.122). Looking further at individual measures of productivity, (Table 3) we find that both language proficiency and years of experience negatively predict developers agreeing that Copilot helps them write better code. However, proficiency positively predicts developers agreeing that Copilot helps them stay in the flow, focus on more satisfying work, spend less effort on repetitive tasks, and perform repetitive tasks faster. Years of experience negatively predicts developers feeling less frustrated in coding sessions and perform repetitive tasks faster while using Copilot, but positively predicts developers making progress faster when working in an unfamiliar language. These findings suggest that experienced developers who are already highly skilled are less likely to write better code with Copilot, but Copilot can assist their productivity in other ways particularly when engaging with new areas and automating routine work. Experienced developers who are already highly skilled are less likely to write better code with Copilot, but Copilot can assist their productivity in other ways. Table 3. Effects of experience on facets of productivity where result of linear regression was a statistically significant covariate. productivity measure coeff proficiency better_code – 0 . 061 * proficiency stay_in_flow 0 . 069 * proficiency focus_satisfying 0 . 067 * proficiency less_effort_repetitive 0 . 072 ** proficiency repetitive_faster 0 . 055 *** years better_code – 0 . 087 * years less_frustrated – 0 . 103 ** years repetitive_faster – 0 . 054 * years unfamiliar_progress 0 . 081 * (*: p < 0.05, **: p < 0.01, ***: p < 0.001.) Table 4. Correlations of acceptance rate with aggregate productivity broken down by subgroup. subgroup coeff n none 0 . 135 * 344 ≤ 2y 0 . 178 ** 451 3 – 5 y 0 . 255 *** 358 6 – 10 y 0 . 265 *** 251 11 – 15 y 0 . 171 * 162 ≥ 16 y 0 . 153 * 214 JavaScript 0 . 227 *** 1184 TypeScript 0 . 165 *** 654 Python 0 . 172 *** 716 other 0 . 178 *** 1829 Junior developers not only report higher productivity gains; they also tend to accept more suggestions. However, the connection observed in section “What Drives Perceived Productivity” is not solely due to differing experience levels. In fact, the connection persists in every single experience group, as shown in Figure 5. Figure 4. Different metrics clustering in latent structures predicting perceived productivity. We color the following groups: flawless suggestions (counting the number of unchanged suggestions), persistence rate (ratio of accepted suggestions that are unchanged), and fuzzy persistence rate (accepted suggestions that are mostly unchanged). Figure 5. Linear regressions between acceptance rate and aggregate productivity by subgroup defined through years of professional experience or programming language use. Dashed lines denote averages. The x-axis is clipped at (0, 0.5), and 95% of respondents fall into that range. Variation over Time Its connection to perceived productivity motivates a closer look at the acceptance rate and what factors influence it. Acceptance rate typically increases over the board when the model or underlying prompt-crafting techniques are improved. But even if these conditions are held constant (the study period did not see changes to either), there are more fine-grained temporal patterns emerging. For coherence of the cultural implications of time of day and weekdays, all data in this section was restricted to users from the U.S. (whether in the survey or not). We used the same time frame as for the investigation in the previous section. In the absence of more fine-grained geolocation, we used the same time zone to interpret timestamps and for day boundaries (Pacific Standard Time), recognizing that this will introduce some level of noise due to the inhomogeneity of U.S. time zones. Nevertheless, we observe strong regular patterns in overall acceptance rate (Figure 6). These lead us to distinguish three different time regimes, all of which are statistically significantly distinct at p<0.001% (using bootstrap resampling): Figure 6. Average acceptance rate during the week. Each point represents the average for a one-hour period, whereas the shaded ribbon shows the min-max variation during the observed four-week period. The weekend: Saturdays and Sundays, where the average acceptance rate is comparatively high at 23.5%. Typical non-working hours during the week: evenings after 4:00 pm PST until mornings 7:00 am PST, where the average acceptance rate is also rather high at 23%. Typical working hours during the week from 7:00 am PST to 4:00 pm PST, where the average acceptance rate is much lower at 21.2%. Conclusions When we set out to connect the productivity benefit of GitHub Copilot to usage measurements from developer activity, we collected measurements about acceptance of completions in line with prior work, but also developed persistence metrics, which arguably capture sustained and direct impact on the resulting code. We were surprised to find acceptance rate (number of acceptances normalized by the number of shown completions) to be better correlated with reported productivity than our measures of persistence. In hindsight, this makes sense. Coding is not typing, and GitHub Copilot’s central value lies not in being the way the user enters most of their code. Instead, it lies in helping the user to make the best progress toward their goals. A suggestion that serves as a useful template to tinker with may be as good or better than a perfectly correct (but obvious) line of code that only saves the user a few keystrokes. This suggests that a narrow focus on the correctness of suggestions would not tell the whole story for these kinds of tooling. Instead one could view code suggestions inside an IDE to be more akin to a conversation. While chatbots such as ChatGPT are already used for programming tasks, they are explicitly structured as conversations. Here, we hypothesize that interactions with Copilot, which is not a chatbot, share many characteristics with natural-language conversations. We see anecdotal evidence of this in comments posted about GitHub Copilot online (see Appendix E for examples) in which users talk about sequences of interactions. A conversation turn in this context consists of the prompt in the completion request and the reply as the completion itself. The developer’s response to the completion arises from the subsequent changes which are incorporated in the next prompt to the model. There are clear programming parallels to factors such as specificity and repetition that have been identified to affect human judgements of conversation quality.18 Researchers have already investigated the benefits of natural-language feedback to guide program synthesis,2 so the conversational framing of coding completions is not a radical proposal. But neither is it one we have seen followed yet. References 1. Amann, S., Proksch, S., Nadi, S., and Mezini, M. A study of visual studio usage in practice. In IEEE 23rd Intern. Conf. on Software Analysis, Evolution, and Reengineering 1 . IEEE Computer Society, (March 2016), 124–134; 10.1109/SANER.2016.39 2. Austin, J. et al. Program synthesis with large language models. CoRR abs/2108.07732 (2021); https://arxiv.org/abs/2108.07732 3. Ari Aye, G., Kim, S., and Li, H. Learning autocompletion from real-world datasets. In Proceedings of the 43rd IEEE/ACM Intern. Conf. on Software Engineering: Software Engineering in Practice, (May 2021), 131–139; 10.1109/ICSE-SEIP52600.2021.00022 4. Beller, M., Orgovan, V., Buja, S., and Zimmermann, T. Mind the gap: On the relationship between automatically measured and self-reported productivity. IEEE Software 38, 5 (2020), 24–31. 5. Chen, M. et al. Evaluating large language models trained on code. CoRR abs/2107.03374 (2021); https://arxiv.org/abs/2107.03374 6. Forsgren, N. et al. The SPACE of developer productivity: There’s more to it than you think. Queue 19, 1 (2021), 20–48. 7. Hellendoorn, V.J., Proksch, S., Gall, H.C., and Bacchelli, A. When code completion fails: A case study on real-world completions. In Proceedings of the 41st Intern. Conf. on Software Engineering, J.M. Atlee, T. Bultan, and J. Whittle (eds). IEEE / ACM, (May 2019), 960–970; 10.1109/ICSE.2019.00101 8. Hendrycks, D. et al. Measuring coding challenge competence with APPS. CoRR abs/2105.09938, (2021); https://arxiv.org/abs/2105.09938 9. Hindle, A. et al. On the naturalness of software. In 34th Intern. Conf. on Software Engineering, M. Glinz, G.C. Murphy, and M. Pezzè (eds). IEEE Computer Society, June 2012, 837–847; 10.1109/ICSE.2012.6227135 10. Jaspan, C. and Sadowski, C. No single metric captures productivity. Rethinking Productivity in Software Engineering, (2019), 13–20. 11. Kulal, S. et al. Spoc: Search-based pseudocode to code. In Proceedings of Advances in Neural Information Processing Systems 32, H.M.Wallach et al. (eds), Dec. 2019, 11883–11894; https://bit.ly/3H7YLtF 12. Meyer, A.N., Barr, E.T., Bird, C., and Zimmermann, T. Today was a good day: The daily life of software developers. IEEE Transactions on Software Engineering 47, 5 (2019), 863–880. 13. Meyer, A.N. et al. The work life of developers: Activities, switches and perceived productivity. IEEE Transactions on Software Engineering 43, 12 (2017), 1178–1193. 14. Meyer, A.N., Fritz, T., Murphy, G.C., and Zimmermann, T. Software developers’ perceptions of productivity. In Proceedings of the 22nd ACM SIGSOFT Intern. Symp. on Foundations of Software Engineering (2014), 19–29. 15. Murphy-Hill, E. et al. What predicts software developers’ productivity? IEEE Transactions on Software Engineering 47, 3 (2019), 582–594. 16. Peng, S., Kalliamvakou, E., Cihon, P., and Demirer, M. The impact of AI on developer productivity: Evidence from GitHub Copilot. arXiv:2302.06590 [cs.SE] (2014) 17. Ramírez, Y.W. and Nembhard, D.A. Measuring knowledge worker productivity: A taxonomy. J. of Intellectual Capital 5, 4 (2004), 602–628. 18. See, A., Roller, S., Kiela, D., and Weston, J. What makes a good conversation? How controllable attributes affect human judgments. In Proceedings of the 2019 Conf. of the North American Chapter of the Assoc. for Computational Linguistics: Human Language Technologies 1, J. Burstein, C. Doran, and T. Solorio (eds). Assoc. for Computational Linguistics, (June 2019), 1702–1723; 10.18653/v1/n19-1170 19. Storey, M. et al. Towards a theory of software developer job satisfaction and perceived productivity. In Proceedings of the IEEE Trans. on Software Engineering 47, 10 (2019), 2125–2142. 20. Svyatkovskiy, A., Deng, S.K., Fu, S., and Sundaresan, N. Intellicode compose: Code generation using transformer. In Proceedings of the 28th ACM Joint European Software Eng. Conf. and Symp. on the Foundations of Software Eng., P. Devanbu, M.B. Cohen, and T. Zimmermann (eds). ACM, (Nov. 2020), 1433–1443; 10.1145/3368089.3417058 21. Svyatkovskiy, A. et al. Fast and memory-efficient neural code completion. In Proceedings of the 18th IEEE/ACM Intern. Conf. on Mining Software Repositories, (May 2021, 329–340; 10.1109/MSR52588.2021.00045 22. Vaithilingam, P., Zhang, T., and Glassman, E. Expectation vs. experience: Evaluating the usability of code generation tools powered by large language models. In Proceedings of the 2022 Conf. on Human Factors in Computing Systems. 23. Vaithilingam, P., Zhang, T., and Glassman, E.L. Expectation vs. experience: Evaluating the usability of code generation tools powered by large language models. In Proceedings of the CHI Conf. on Human Factors in Computing Systems, Association for Computing Machinery, Article 332 (2022), 7; 10.1145/3491101.3519665 24. Wagner, S. and Ruhe, M. A systematic review of productivity factors in software development. arXiv preprint arXiv:1801.06475 (2018). 25. Wang, D. et al. From human-human collaboration to human-AI collaboration: Designing AI systems that can work together with people. In Proceedings of the 2020 CHI Conf. on Human Factors in Computing Systems (2020), 1–6. 26. Weisz, J.D. et al. Perfection not required? Human-AI partnerships in code translation. In Proceedings of the 26th Intern. Conf. on Intelligent User Interfaces, T. Hammond et al. (eds). ACM, (April 2021), 402–412; 10.1145/3397481.3450656 27. Winters, T., Manshreck, T., and Wright, H. Software Engineering at Google: Lessons Learned from Programming Over Time . O’Reilly Media (2020). 28. Wold, S., Sjöström, M., and Eriksson, L. PLS-regression: A basic tool of chemometrics. Chemometrics and Intelligent Laboratory Systems 58, 2 (2001), 109–130; 10.1016/S0169-7439(01)00155-1. 29. Zhou, W., Kim, S., Murali, V., and Ari Aye, G. Improving code autocompletion with transfer learning. CoRR abs/2105.05991 (2021); https://arxiv.org/abs/2105.05991 Footnotes a Nevertheless, such completion times are greatly reduced in many settings, often by more than half.16 b Note that these values are specific to the time period in our study (early 2022). Both figures have since increased because of model and prompting improvements. c See https://bit.ly/3S7oqZV. d Appendices can be found in the arXiv version https://arxiv.org/pdf/2205.06537.pdf. About the Authors Albert Ziegler is a principal researcher at GitHub, Inc., San Francisco, USA. Eirini Kalliamvakou is a staff researcher at GitHub, Inc., San Francisco, USA. X. Alice Li is a staff researcher for Machine Learning at GitHub, San Francisco, USA. Andrew Rice is a principal researcher at GitHub, Inc., San Francisco, USA. Devon Rifkin is a principal research engineer at GitHub, Inc., San Francisco, USA. Shawn Simister is a staff software engineer at GitHub, Inc., San Francisco, USA. Ganesh Sittampalam is a principal software engineer at GitHub, Inc., San Francisco, USA. Edward Aftandilian is a principal researcher at GitHub, Inc., San Francisco, USA. Share Twitter Reddit Hacker News Print Join the Discussion Submit an Article to CACM CACM welcomes unsolicited submissions on topics of relevance and value to the computing community. You Just Read Measuring GitHub Copilot’s Impact on Productivity View in the ACM Digital Library This work is licensed under a Creative Commons Attribution International 4.0 License. DOI 10.1145/3633453 March 2024 Issue Vol. 67 No. 3 Pages: 54-63 Table of Contents Related Reading Practice Taking Flight with Copilot Computing Applications Research and Advances Computing Education in the Era of Generative AI Artificial Intelligence and Machine Learning News AI Rewrites Coding Artificial Intelligence and Machine Learning Opinion The Premature Obituary of Programming Computing Applications Advertisement Advertisement Join the Discussion (0) Become a Member or Sign In to Post a Comment Sign In Sign Up The Latest from CACM Explore More News Mar 17 2025 Digital Twins Promise Personalized Medical Care Tim Hornyak Architecture and Hardware BLOG@CACM Mar 13 2025 The UGC Overload: Scaling Content Moderation for Massive Datasets Alex Tray Data and Information BLOG@CACM Mar 12 2025 Multi-Dimensional Growth of GenAI Use by STEM Undergraduates Yael Erez and Orit Hazzan Artificial Intelligence and Machine Learning Shape the Future of Computing ACM encourages its members to take a direct hand in shaping the future of the association. There are more ways than ever to get involved. Get Involved Communications of the ACM (CACM) is now a fully Open Access publication. By opening CACM to the world, we hope to increase engagement among the broader computer science community and encourage non-members to discover the rich resources ACM has to offer. Learn More CACM on Twitter CACM on Reddit CACM on LinkedIn Topics Architecture and Hardware Artificial Intelligence and Machine Learning Computer History Computing Applications Computing Profession Data and Information Education HCI Philosophy of Computing Security and Privacy Society Software Engineering and Programming Languages Systems and Networking Theory Magazine Latest Issue Magazine Archive Editorial Staff and Board Submit an Article Alerts & Feeds Author Guidelines Communications of the ACM About Us Frequently Asked Questions Contact Us For Advertisers Join ACM © 2025 Communications of the ACM. All Rights Reserved. Cookie Notice Privacy Policy

---


# TITLE: Clash of the AI Pair Programmers — Github Copilot vs Cursor AI — Initial Impressions | by jacob binny | Medium
# URL: https://medium.com/@jacobbinny/clash-of-the-ai-pair-programmers-github-copilot-vs-cursor-ai-initial-ff649ba0db68
# CONTENT:
Clash of the AI Pair Programmers — Github Copilot vs Cursor AI — Initial Impressions | by jacob binny | Medium Open in app Sign up Sign in Write Sign up Sign in Home Library Stories Stats Clash of the AI Pair Programmers — Github Copilot vs Cursor AI — Initial Impressions jacob binny · Follow 4 min read · Nov 9, 2024 -- Listen Share In this post, I will be talking about initial impressions of using GitHub Copilot and Cursor AI for my daily coding activities. I have been programming and writing code in C#, Java, Python, Typescript and a bit of Kotlin for quite while now (mainly with C# and Java for over 15 years), so the thought of having AI coding systems help in making me write code was exciting. I was a bit apprehensive of whether it was all a marketing gimmick or whether there was any truth of it making my life easier. Although, I was not an early adopter of it due to my skepticism, the curiosity finally got the better of me and I started dabbling with it. First Impressions GitHub Copilot I started using with Copilot as it was the first on the scene having been released in October 2021 and the most widely used. The most interesting feature for me was that it could integrate with my existing IDEs that I use most — Visual Studio Code, Visual Studio and Android Studio. I started of by trying it on small scripts and programs which I use for rare occasions. I was pleasantly surprised by the quality of the code recommendations. It also did help me with my primary goal, to reduce coding time. As an example, I had to provide a productivity report to my boss which showed a comparison on the time taken by a resource in my team to complete items in a sprint against the lines of code written for that period. I had to find a correlation between the number of tasks completed via JIRA against the code commits in git. I decided to write a python script to get this data. When I started this activity, I was not using Copilot, so, I went with the traditional approach of reading through the docs and finding apis to get this data and combine it, but it was taking quite a while. Around 3 days in and I was still not nearing completion. Suddenly, my boss asked for the report by the next day and I was in a bit of a spot. So, I decided to see whether I could use Copilot. I took a trial subscription and then started creating the script with prompts via Copilot. To my surprise (and delight), I was able to finish the whole script in 45 minutes!! Talk about a productivity boost. It did help that I was clear on what was needed and so had a good idea on the prompt to be provided, but still it was impressive. I became a firm believer. The other thing I was happy with Copilot was its integration with Android Studio. I do not like the IDE and believe that using it for coding is pretty painful. But Copilot is nicely integrated with it and the code suggestions were quite helpful with that. Cursor AI Cursor AI is a relatively new entrant to this race, but it has gained quite a good amount of traction. I first came to know it via an tech reel in Instagram where it was touted as the Copilot killer. That was pretty bold claim and I was not sure that someone could challenge GitHub AI as the amount of training material GitHub has should be unmatched. Also, I felt a bit apprehensive as this was a separate editor and not plugged into our existing IDEs. The only reason I decided to give in was because Cursor is a fork of my all time favorite IDE — Visual Studio Code. I took a trial account and setup the IDE. I started off again with sample scripts written in python and bash with Cursor. Again, the code completion and suggestions seemed quite good and comparable to Copilot. What I did find quite enjoyable was the fact that having the AI as part of the IDE and not as a plugin did really help the usage. For starters, the auto complete felt much faster (although I didn’t do any benchmarks on the comparison) and the suggestions had more context from the project as a whole. For e.g. if I have made an edit on one of the files to update a log config, then in the next file, it would show me suggestions to make the same changes. So, the refactoring of code was much easier and quicker. Also, the tab option to make changes to the entire file kind of like find and replace was a boon. So, if I want to say change all the print() statements in my file to logger, it was easier. Also, based on the location of the print statement, cursor would suggest whether it should be an info or error log. I did try Cursor with a larger code base and found the results satisfactory. During the time of my tests, Cursor had the advantage of giving support for multiple models as opposed to Copilot, but Copilot’s newer version does support this. Conclusion I had used both the systems for close to 3 months each and have found both to be really good tools that every developer should choose. The choice of which one to go with (or whether to go with both) lies with the individual as there is no clear winner. The costing for Copilot does make it a bit more attractive though. Cursor AI is my current choice of development tool although I have recommended both to my peers. Advantages of Copilot over Cursor AI Has wider industry usage and support Support for existing IDEs would be good to developers who do not want to move away from their IDE of choice (The plugin for Eclipse does not work as expected though) Better for smaller scripts and projects Half the price of Cursor Advantages of Cursor AI over Copilot Better context aware results on larger projects Since it is part of the IDE, the usage is much more seamless Switch will not be difficult for developers on Visual Studio Code GitHub Copilot — https://github.com/features/copilot Cursor AI — https://www.cursor.com/ Happy Coding!!! Ai Pair Programmer Github Copilot Cursor -- -- Follow Written by jacob binny 5 Followers ·1 Following Follow No responses yet Help Status About Careers Press Blog Privacy Terms Text to speech Teams

---


